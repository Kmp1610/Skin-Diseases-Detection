{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba5df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras import layers as L\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Model\n",
    "from keras.layers import Input\n",
    "import tensorflow_addons as tfa \n",
    "import random\n",
    "from PIL import Image\n",
    "from pathlib import Path, PurePath\n",
    "import splitfolders\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV3Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc492c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c282b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 8\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fcea623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 680 images belonging to 8 classes.\n",
      "Found 244 images belonging to 8 classes.\n",
      "Found 233 images belonging to 8 classes.\n",
      "Number of images in the training dataloader: 680\n",
      "Number of images in the validation dataloader: 244\n",
      "Number of images in the testing dataloader: 233\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define your data directories\n",
    "train_dir = r\"C:\\Users\\Vansh\\Desktop\\projects\\FS,RS\\skin-disease-dataset\\Training\"  # Location of the Training Data\n",
    "test_dir = r\"C:\\Users\\Vansh\\Desktop\\projects\\FS,RS\\skin-disease-dataset\\test_set\"\n",
    "val_dir = r\"C:\\Users\\Vansh\\Desktop\\projects\\FS,RS\\skin-disease-dataset\\validation\"\n",
    "\n",
    "# Create image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator( rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# Flow training images in batches using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Flow validation images in batches using val_test_datagen generator\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Flow test images in batches using val_test_datagen generator\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Check the number of images in the generators\n",
    "num_train_images = train_generator.samples\n",
    "print(f\"Number of images in the training dataloader: {num_train_images}\")\n",
    "\n",
    "num_val_images = val_generator.samples\n",
    "print(f\"Number of images in the validation dataloader: {num_val_images}\")\n",
    "\n",
    "num_test_images = test_generator.samples\n",
    "print(f\"Number of images in the testing dataloader: {num_test_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad6dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32, 8)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = train_generator.next()\n",
    "print(train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8741580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32, 8)\n"
     ]
    }
   ],
   "source": [
    "val_data, val_labels = val_generator.next()\n",
    "print(val_data.shape, val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559de1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32, 8)\n"
     ]
    }
   ],
   "source": [
    "test_data, test_labels = test_generator.next()\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98696057",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "    classes=8,\n",
    "    classifier_activation='softmax',\n",
    "    weights='imagenet', \n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf8cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11723f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 62720)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 501768    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,759,752\n",
      "Trainable params: 501,768\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobmodel1 = tf.keras.models.Sequential()\n",
    "\n",
    "mobmodel1.add(model)\n",
    "mobmodel1.add(tf.keras.layers.Flatten())\n",
    "mobmodel1.add(tf.keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "mobmodel1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "477c8bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 16s 398ms/step - loss: 7.7810 - accuracy: 0.4632 - val_loss: 3.3882 - val_accuracy: 0.7459\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 8s 366ms/step - loss: 2.1517 - accuracy: 0.7868 - val_loss: 2.4076 - val_accuracy: 0.7746\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 1.5202 - accuracy: 0.8412 - val_loss: 2.1975 - val_accuracy: 0.8156\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 7s 306ms/step - loss: 1.0566 - accuracy: 0.8721 - val_loss: 1.7177 - val_accuracy: 0.8361\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.8621 - accuracy: 0.8912 - val_loss: 2.4164 - val_accuracy: 0.8279\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 7s 310ms/step - loss: 1.4238 - accuracy: 0.8574 - val_loss: 2.0914 - val_accuracy: 0.8156\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 7s 319ms/step - loss: 1.0409 - accuracy: 0.8926 - val_loss: 1.9676 - val_accuracy: 0.8279\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 8s 358ms/step - loss: 0.7909 - accuracy: 0.9044 - val_loss: 1.5063 - val_accuracy: 0.8074\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 7s 303ms/step - loss: 0.8129 - accuracy: 0.9088 - val_loss: 1.9242 - val_accuracy: 0.8443\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 7s 302ms/step - loss: 0.4960 - accuracy: 0.9279 - val_loss: 1.1188 - val_accuracy: 0.9016\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 7s 299ms/step - loss: 0.6346 - accuracy: 0.9368 - val_loss: 2.5917 - val_accuracy: 0.8115\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 7s 297ms/step - loss: 1.0115 - accuracy: 0.9132 - val_loss: 1.4145 - val_accuracy: 0.8770\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 7s 300ms/step - loss: 0.9362 - accuracy: 0.9103 - val_loss: 3.1298 - val_accuracy: 0.7869\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 7s 305ms/step - loss: 1.4619 - accuracy: 0.8868 - val_loss: 1.8851 - val_accuracy: 0.8689\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 6s 295ms/step - loss: 0.8786 - accuracy: 0.9206 - val_loss: 2.6360 - val_accuracy: 0.8361\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 7s 303ms/step - loss: 0.4384 - accuracy: 0.9618 - val_loss: 2.0079 - val_accuracy: 0.8607\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 6s 289ms/step - loss: 0.5663 - accuracy: 0.9485 - val_loss: 2.6268 - val_accuracy: 0.8279\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 6s 295ms/step - loss: 0.4619 - accuracy: 0.9485 - val_loss: 2.3382 - val_accuracy: 0.8525\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.3504 - accuracy: 0.9588 - val_loss: 1.8717 - val_accuracy: 0.8934\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 6s 289ms/step - loss: 0.5529 - accuracy: 0.9456 - val_loss: 2.3591 - val_accuracy: 0.8607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1876ae51190>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobmodel1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics='accuracy',\n",
    ")\n",
    "mobmodel1.fit(train_generator,validation_data= val_generator,verbose=1,epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "646ce188",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\Users\\Vansh\\Desktop\\projects\\model_fsd4.h5\"\n",
    "mobmodel1.save(filepath, overwrite=True, save_format=\"tf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from keras.models import load_model\n",
    "model= load_model(\"model_fsd.h5\")\n",
    "converter= tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "lite_model= converter.convert()\n",
    "with open(\"lite_model.tflite\",\"wb\") as f:\n",
    "    f.write(lite_model)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f250f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
